## 1â€¯Â·â€¯Why â€œ4â€‘point correlationâ€ is weak & what you can do today

| Fact                                                                      | Implication                                                                   |
| ------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| Pearson/Spearman correlation needs many degreesâ€‘ofâ€‘freedom to be reliable | With **nâ€¯=â€¯4** cells, dfâ€¯=â€¯2 â‡’ even Ïâ€¯=â€¯0.90 is *not* â€œsignificantâ€ (pâ€¯â‰ˆâ€¯0.1) |
| A single outlier flips the sign                                           | The direction (+/â€“) becomes arbitrary                                         |
| Correlation only measures linear coâ€‘movement                              | It ignores level differences that matter for rootâ€‘cause                       |

### ðŸ”§ A dropâ€‘in alternative you can ship now

**Step 1â€¯â€“â€¯Directional screen (still required by TL/manager)**

```text
IF sign(metric_delta) == sign(hypo_metric_delta) âžž passes direction check
ELSE âžž fails
```

**Step 2â€¯â€“â€¯Size screen (replace correlation)**
Compute **composition\_gap** and **performance\_gap**:

```
composition_gap = Î£ mix_region_i Â· rate_row_i  â€“  rate_row_overall
performance_gap = observed_rate_region        â€“  Î£ mix_region_i Â· rate_row_i
```

> â€¢ If |composition\_gap| > X% of global mean â†’ conclude â€œmix likely driverâ€
> â€¢ Else if |performance\_gap| > Y% of global mean â†’ conclude â€œefficiency likely driverâ€
> â€¢ Else â€œweak evidenceâ€

Set X,â€¯Y as simple thresholds (e.g., 5â€¯p.p. of winâ€‘rate).
No bootstrap needed; youâ€™ve turned the decision into deterministic ruleâ€‘based flagsâ€”easy to implement and explain.

*You still record the absolute numbers in payload so later, when you have more historical data, you can run proper statistics.*

---

## 2â€¯Â·â€¯Kitagawa / Oaxacaâ€‘Blinder decompositionâ€”plainâ€‘English walkâ€‘through

Imagine each initiative is bucketed by AM level (`i = L4, L5, L6, â€¦`).
For a given region **R** vs **Restâ€‘ofâ€‘World (ROW)**:

| Symbol       | Meaning                                       |
| ------------ | --------------------------------------------- |
| `mix_R_i`    | Share of initiatives in cellâ€¯*i* for region R |
| `rate_R_i`   | Winâ€‘rate in cellâ€¯*i* for region R             |
| `mix_ROW_i`  | Share for ROW                                 |
| `rate_ROW_i` | Winâ€‘rate for ROW                              |

### Total rate difference

```
Î” = ObservedRate_R  â€“  ObservedRate_ROW
  = Î£ mix_R_i * rate_R_i  â€“  Î£ mix_ROW_i * rate_ROW_i
```

### Decompose into two parts

1. **Composition effect (mix difference, rates fixed at ROW):**

   ```
   Î”_mix = Î£ (mix_R_i â€“ mix_ROW_i) * rate_ROW_i
   ```

   > *â€œIf region R had ROWâ€™s winâ€‘rates but kept its own mix, this is the gap.â€*

2. **Performance (efficiency) effect (rate difference, mix fixed at R):**

   ```
   Î”_perf = Î£ mix_R_i * (rate_R_i â€“ rate_ROW_i)
   ```

   > *â€œIf region R kept its mix but used its own rates, this is the gap.â€*

By construction: `Î” = Î”_mix + Î”_perf`.

### Interpretation

* **Î”\_mix large** â†’ book or AMâ€‘level composition explains most of the anomaly.
* **Î”\_perf large** â†’ same mix, but R converts better/worse â‡’ execution or efficiency issue.

You can compute both gaps with only the four buckets (L4â€“L7) **without correlation**.
Store them and compare magnitudes; whichever dominates becomes your â€œrootâ€‘cause flag.â€

---

### ðŸ”‘  Minimal payload you store

```json
"payload": {
  "composition_gap": 0.005,      // +0.5â€¯pp vs ROW
  "performance_gap": 0.020,      // +2.0â€¯pp
  "dominant_driver": "performance",  // simple if/else rule
  "cell_table": [
    {"level":"L4","mix_R":0.18,"mix_ROW":0.12,"rate_R":0.025,"rate_ROW":0.022},
    ...
  ]
}
```

Downstream systems (Slides, Unidash, LLM) can read:

* Direction check: `dominant_driver`
* Magnitude: `composition_gap`, `performance_gap`
* Detail table for drillâ€‘down

No correlation needed, and the logic is transparent and reproducible.



### How to handle (and communicate) â€œmixedâ€ or conflicting results across regions

| Situation                                                                  | What you surface                                                                                                              | Why it avoids followâ€‘up churn                                                                                                                       |
| -------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Region under review shows Î”\_mixâ€¯>â€¯Î”\_perf**                             | â€œPrimary driver: *composition* (book/AM mix). Composition explains **80â€¯%** of this regionâ€™s gap.â€                            | Gives a single headline + shows share, so stakeholders know itâ€™s dominant, not exclusive.                                                           |
| **Another region shows the opposite pattern**                              | For that region you still report its own dominant driver; your system does **not** compare regionsâ€™ drivers unless requested. | Rootâ€‘cause analysis is **per region**; conflicting signs arenâ€™t a contradictionâ€”they simply mean different regions are driven by different factors. |
| **Both effects similar in magnitude** (e.g., 0.02 vsâ€¯0.025)                | Flag â€œno single dominant driverâ€ and list both gaps.                                                                          | Preâ€‘empts questions by acknowledging ambiguity instead of hiding it.                                                                                |
| **Stakeholder asks â€œwhy NA is mixâ€‘driven but EMEA is efficiencyâ€‘driven?â€** | Show the cell detail table; theyâ€™ll see NAâ€™s mix skewed to senior AMs, EMEAâ€™s rates lower in every cell.                      | Transparent evidence defuses followâ€‘up.                                                                                                             |

---

### Concrete rules you can implement

```python
share_mix  = abs(Î”_mix)  / abs(Î”_total)
share_perf = abs(Î”_perf) / abs(Î”_total)

if share_mix >= 0.7:
    dominant = "composition"
elif share_perf >= 0.7:
    dominant = "performance"
else:
    dominant = "mixed"
```

Store in payload:

```json
"composition_gap": 0.005,
"performance_gap": 0.020,
"dominant_driver": "performance",   // or "mixed"
"share_of_total": {"composition": 0.2, "performance": 0.8}
```

Downstream narrative template:

```
*Primary driver for AMâ€‘NA: performance (80â€¯% of gap).*
Book mix accounts for 20â€¯%; both factors shown below.
```

---

### Why this stops endless â€œwhatâ€‘aboutâ€ questions

1. **Regionâ€‘specific headline** â€“ Each reader sees the main takeaway for their scope.
2. **Percentage breakdown** â€“ Quantifies how sharp the difference is (or isnâ€™t).
3. **Cellâ€‘level evidence** â€“ Drillâ€‘down table is there if deeper investigation is needed.
4. **Consistent rule** â€“ Same 70â€¯% threshold used everywhere; no adâ€‘hoc judgement.

Youâ€™ll rarely be asked â€œbut EMEAâ€™s numbers look differentâ€ because the payload already tells them *why* and *by how much*.


### When **Î”\_mix** and **Î”\_perf** have opposite signs

(*e.g.,â€¯composition gapâ€¯=â€¯+0.02â€¯pp, performance gapâ€¯=â€¯â€“0.05â€¯pp*)

---

#### 1â€¯Â·â€¯Interpretation

| Case                               | Meaning                                                                                                                 |
| ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Î”\_mixâ€¯>â€¯0** (pushes KPI *up*)   | The regionâ€™s book / AMâ€‘level mix is **favourable** relative to ROW.                                                     |
| **Î”\_perfâ€¯<â€¯0** (pulls KPI *down*) | Given that mix, the region converts **worse** than ROW in every cell.                                                   |
| **Total Î” = Î”\_mixâ€¯+â€¯Î”\_perf**     | If signs differ, the two effects **partially cancel**. The observed anomaly is whichever effect dominates in magnitude. |

---

#### 2â€¯Â·â€¯Actionable rule

```python
if sign(Î”_mix) != sign(Î”_perf):
    dominant = "offsetting"
    explanation = (
        "Composition skews positive (+{:.2p}) but is more than offset by "
        "lower withinâ€‘cell performance (â€“{:.2p})."
    ).format(abs(Î”_mix), abs(Î”_perf))
else:
    # previous 70â€¯% rule
```

---

#### 3â€¯Â·â€¯Payload example

```json
"composition_gap": 0.020,
"performance_gap": -0.050,
"dominant_driver": "offsetting",
"share_of_total": {
  "composition": 0.29,
  "performance": 0.71   // in absolute terms
},
"explanation": "Favourable AM mix adds 2â€¯pp to winâ€‘rate, but lower execution subtracts 5â€¯pp; net effect â€“3â€¯pp."
```

---

#### 4â€¯Â·â€¯Narrative template

> **Offsetting effects:**
> Regionâ€¯{{region}}â€™s seniorâ€‘AM mix would raise winâ€‘rate by **+2â€¯pp**, but withinâ€‘cell efficiency is **â€“5â€¯pp** vs benchmark, producing the net decline.

---

#### 5â€¯Â·â€¯Why this matters

* **Stakeholders see both levers.** They may keep the mix but fix execution.
* **No confusion about sign.** You explicitly tell them the forces cancel.

---

#### 6â€¯Â·â€¯Optional visual

A stacked bar:

```
+2â€¯pp  (composition)   â–®â–®
â€“5â€¯pp  (performance)   â–®â–®â–®â–®â–®
-----------------------------
Net â€“3â€¯pp
```

---

**Bottomâ€‘line:**
Oppositeâ€‘sign gaps are perfectly valid; treat them as *offsetting* and highlight which one overrides the other.
